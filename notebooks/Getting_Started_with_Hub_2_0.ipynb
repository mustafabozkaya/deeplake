{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lKU8kmSs65xv",
        "G-DM6PKq_di2",
        "46H4nEnZDv5m",
        "JGo-E8Z8Ho6F",
        "K385Fpvmqc0l",
        "NQipSo2OF_lB",
        "LVma__gxGq97",
        "lNPlnk7V58qE",
        "zTsEinAP7HVg",
        "vUQR2ToB8yWF",
        "guao84xTb4Zg",
        "iXRCphquSFs3",
        "vBM1DKntaXwS"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mustafabozkaya/deeplake/blob/main/notebooks/Getting_Started_with_Hub_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKU8kmSs65xv"
      },
      "source": [
        "# **Step 1**: _Hello World_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrjGQON37lk2"
      },
      "source": [
        "## Installing Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pcfYcPu7KxY"
      },
      "source": [
        "Hub can be installed via `pip`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC_N5qOx6o0d"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip3 install hub\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**By default, Hub does not install dependencies for audio, video, and google-cloud (GCS) support. They can be installed using:**"
      ],
      "metadata": {
        "id": "z4_rfJ_GVxLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install \"hub[av]\"    -> Video and Audio support via PyAV\n",
        "\n",
        "#pip install \"hub[gcp]    -> GSS support via google-* dependencies\n",
        "\n",
        "#pip install \"hub[all]\"   -> Installs everything - audio, video and GCS support"
      ],
      "metadata": {
        "id": "AwpEic3jV2nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N-f2SYU7OjQ"
      },
      "source": [
        "## Fetching your first Hub dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aNFn7rZ7qxP"
      },
      "source": [
        "Begin by loading in [MNIST](https://en.wikipedia.org/wiki/MNIST_database), the hello world dataset of machine learning. \n",
        "\n",
        "First, load the `Dataset` by pointing to its storage location. Datasets hosted on the Activeloop Platform are typically identified by the namespace of the organization followed by the dataset name: `activeloop/mnist-train`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izccjS4k7NvX"
      },
      "source": [
        "import hub\n",
        "\n",
        "dataset_path = 'hub://activeloop/mnist-train'\n",
        "ds = hub.load(dataset_path) # Returns a Hub Dataset but does not download data locally"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR5n8yYg-0Wu"
      },
      "source": [
        "## Reading Samples From a Hub Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdaAKaS-3NO"
      },
      "source": [
        "Data is not immediately read into memory because Hub operates [lazily](https://en.wikipedia.org/wiki/Lazy_evaluation). You can fetch data by calling the `.numpy()` method, which reads data into a NumPy array.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qpQeNoq-xfo"
      },
      "source": [
        "# Indexing\n",
        "img = ds.images[0].numpy()              # Fetch the 1st image and return a NumPy array\n",
        "label = ds.labels[0].numpy(aslist=True) # Fetch the 1st label and store it as a \n",
        "                                        # as a list\n",
        "                              \n",
        "text_label = ds.labels.info.class_names[ds.labels[0].numpy()[0]]\n",
        "\n",
        "# Slicing\n",
        "imgs = ds.images[0:100].numpy() # Fetch 100 images and return a NumPy array\n",
        "                                # The method above produces an exception if \n",
        "                                # the images are not all the same size\n",
        "\n",
        "labels = ds.labels[0:100].numpy(aslist=True) # Fetch 100 labels and store \n",
        "                                             # them as a list of NumPy arrays"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNGHXfdKwJ7W"
      },
      "source": [
        "print('label is {}'.format(label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmi2w0_e_LtH"
      },
      "source": [
        "Congratulations, you've got Hub working on your local machine! ðŸ¤“"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-DM6PKq_di2"
      },
      "source": [
        "# **Step 2**: _Creating Hub Datasets_\n",
        "*Creating and storing Hub Datasets manually.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEzK8LTe_gJW"
      },
      "source": [
        "Hub datasets can be created using two methods: **Manual creation**, gives you full control over connecting your source data (files, images, etc.) to specific tensors in the Hub dataset. **Automatic creation** enables you to quickly create a hub dataset by letting Hub parse the underlying files into Hub dataset tensors (classification dataset only).\n",
        "\n",
        "You don't have to worry about uploading datasets after you've created them. They are automatically synchronized with [wherever they are being stored](https://docs.activeloop.ai/authentication-overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGXGvKU1qsp1"
      },
      "source": [
        "## Manual Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQk29Mnhqn1V"
      },
      "source": [
        "Let's follow along with the example below to create our first dataset. First, download and unzip the small classification dataset below called the *animals dataset*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDJRrlDP_DsW"
      },
      "source": [
        "# Download dataset\n",
        "from IPython.display import clear_output\n",
        "!wget https://github.com/activeloopai/examples/raw/main/colabs/starting_data/animals.zip\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIQf9cY6_vyn"
      },
      "source": [
        "# Unzip to './animals' folder\n",
        "!unzip -qq /content/animals.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIz-MYImAfCg"
      },
      "source": [
        "The dataset has the following folder structure:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuhZZqVIAqj_"
      },
      "source": [
        "animals\n",
        "- cats\n",
        "  - image_1.jpg\n",
        "  - image_2.jpg\n",
        "- dogs\n",
        "  - image_3.jpg\n",
        "  - image_4.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lez5uCJAto4"
      },
      "source": [
        "Now that you have the data, you can **create a Hub `Dataset`** and initialize its tensors. Running the following code will create a Hub dataset inside of the `./animals_hub` folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtzmT0iBNV23"
      },
      "source": [
        "import hub\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "ds = hub.empty('./animals_hub') # Creates the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ5yt0aaNeP5"
      },
      "source": [
        "Next, let's inspect the folder structure for the source dataset `'./animals'` to find the class names and the files that need to be uploaded to the Hub dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubGLkgG8Njbb"
      },
      "source": [
        "# Find the class_names and list of files that need to be uploaded\n",
        "dataset_folder = './animals'\n",
        "\n",
        "# Find the subfolders, but filter additional files like DS_Store that are added on Mac machines.\n",
        "class_names = [item for item in os.listdir(dataset_folder) if os.path.isdir(os.path.join(dataset_folder, item))]\n",
        "\n",
        "files_list = []\n",
        "for dirpath, dirnames, filenames in os.walk(dataset_folder):\n",
        "    for filename in filenames:\n",
        "        files_list.append(os.path.join(dirpath, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtVSh0FnNmyI"
      },
      "source": [
        "Next, let's **create the dataset tensors and upload metadata**. Check out our page on [Storage Synchronization](https://docs.activeloop.ai/how-hub-works/storage-synchronization) for details about the `with` syntax below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6QDC6caNpiH"
      },
      "source": [
        "with ds:\n",
        "  # Create the tensors with names of your choice.\n",
        "  ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
        "  ds.create_tensor('labels', htype = 'class_label', class_names = class_names)\n",
        "\n",
        "  # Add arbitrary metadata - Optional\n",
        "  ds.info.update(description = 'My first Hub dataset')\n",
        "  ds.images.info.update(camera_type = 'SLR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD-hCSBKBA_m"
      },
      "source": [
        "**Note:** Specifying [htype](https://docs.activeloop.ai/dataset-visualization/data-type-htype) and `dtype` is not required, but it is highly recommended in order to optimize performance, especially for large datasets. Use `dtype` to specify the numeric type of tensor data, and use `htype` to specify the underlying data structure. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR4kLo6YBOhO"
      },
      "source": [
        "Finally, let's **populate the data** in the tensors.         "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QRAyS-HA-Fp"
      },
      "source": [
        "with ds:\n",
        "    # Iterate through the files and append to hub dataset\n",
        "    for file in files_list:\n",
        "        label_text = os.path.basename(os.path.dirname(file))\n",
        "        label_num = class_names.index(label_text)\n",
        "        \n",
        "        #Append data to the tensors\n",
        "        ds.append({'images': hub.read(file), 'labels': np.uint32(label_num)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWqYzfI1DCPG"
      },
      "source": [
        "**Note:** `ds.append({'images': hub.read(path)})` is functionally equivalent to `ds.append({'images': PIL.Image.fromarray(path)})`. However, the `hub.read()` method is significantly faster because it does not decompress and recompress the image if the compression matches the `sample_compression` for that tensor. Further details are available in the next section.\n",
        "\n",
        "**Note:** In order to maintain proper indexing across tensors, `ds.append({...})` requires that you to append to all tensors in the dataset. If you wish to skip tensors during appending, please use `ds.append({...}, skip_ok = True)` or append to a single tensor using `ds.tensor_name.append(...)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzHVb521XSud"
      },
      "source": [
        "Check out the first label and image from this dataset. More details about Accessing Data are available in **Step 4**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " ds.labels.info.class_names[ds.labels[0].numpy()[0]]"
      ],
      "metadata": {
        "id": "tCWJ1RqmSTQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMG2oif0XSDZ"
      },
      "source": [
        "Image.fromarray(ds.images[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can print a summary of the dataset structure using:\n"
      ],
      "metadata": {
        "id": "uPZReEhwu3Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds.summary()"
      ],
      "metadata": {
        "id": "BWbOhUV_u9Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8E_f-eXqy1c"
      },
      "source": [
        "## Automatic Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCjy5dH9q3Gi"
      },
      "source": [
        "The above animals dataset can also be converted to Hub format automatically using 1 line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUtOL7F8q1xB"
      },
      "source": [
        "src = \"./animals\"\n",
        "dest = './animals_hub_auto'\n",
        "\n",
        "ds = hub.ingest(src, dest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6xboPUKrs1l"
      },
      "source": [
        "Image.fromarray(ds.images[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03b3r7owq7o8"
      },
      "source": [
        "**Note**: Automatic creation currently supports image classification datasets where classes are separated by folder, though support for other dataset types is continually being added."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK_wpkYsDdH2"
      },
      "source": [
        "## Creating Tensor Hierarchies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1btlOtBDDe4G"
      },
      "source": [
        "Often it's important to create tensors hierarchically, because information between tensors may be inherently coupledâ€”such as bounding boxes and their corresponding labels. Hierarchy can be created using tensor `groups`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICg3Z1z8CRGN"
      },
      "source": [
        "ds = hub.empty('./groups_test') # Creates the dataset\n",
        "\n",
        "# Create tensor hierarchies\n",
        "ds.create_group('my_group')\n",
        "ds.my_group.create_tensor('my_tensor')\n",
        "\n",
        "# Alternatively, a group can us created using create_tensor with '/'\n",
        "ds.create_tensor('my_group_2/my_tensor') # Automatically creates the group 'my_group_2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE-rWBCkpI9T"
      },
      "source": [
        "Tensors in groups are accessed via:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78s3Oa_jpKXV"
      },
      "source": [
        "ds.my_group.my_tensor\n",
        "\n",
        "#OR\n",
        "\n",
        "ds['my_group/my_tensor']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fhjWZ9hDvKe"
      },
      "source": [
        "For more detailed information regarding accessing datasets and their tensors, check out **Step 4**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46H4nEnZDv5m"
      },
      "source": [
        "# **Step 3**: _Understanding Compression_\n",
        "\n",
        "*Using compression to achieve optimal performance.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ajldDggEp8O"
      },
      "source": [
        "**Data in Hub can be stored in raw uncompressed format. However, compression is highly recommended for achieving optimal performance in terms of speed and storage.**\n",
        "\n",
        "\n",
        "Compression is specified separately for each tensor, and it can occur at the `sample` or `chunk` level. For example, when creating a tensor for storing images, you can choose the compression technique for the image samples using the `sample_compression` input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOw9hc0jDpQY"
      },
      "source": [
        "import hub\n",
        "\n",
        "# Set overwrite = True for re-runability\n",
        "ds = hub.empty('./compression_test', overwrite = True)\n",
        "\n",
        "ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv4ktXoCE2K2"
      },
      "source": [
        "In this example, every image added in subsequent `.append(...)` calls is compressed using the specified `sample_compression` method. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WaFBxrEE9GI"
      },
      "source": [
        "### **Choosing the Right Compression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM8VtZ98FCUu"
      },
      "source": [
        "There is no single answer for choosing the right compression, and the tradeoffs are described in detail in the next section. However, good rules of thumb are:\n",
        "\n",
        "\n",
        "\n",
        "1.   For data that has application-specific compressors (`image`, `audio`, `video`,...), choose the sample_compression technique that is native to the application such as `jpg`, `mp3`, `mp4`,...\n",
        "2.   For other data containing large samples (i.e. large arrays with >100 values), `lz4` is a generic compressor that works well in most applications. `lz4` can be used as a `sample_compression` or `chunk_compression`. In most cases, `sample_compression` is sufficient, but in theory, `chunk_compression` produces slightly smaller data.\n",
        "3.   For other data containing small samples (i.e. labels with <100 values), it is not necessary to use compression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hotuAwslFbAu"
      },
      "source": [
        "### **Compression Tradeoffs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrWvN558v4xn"
      },
      "source": [
        "**Lossiness -** Certain compression techniques are lossy, meaning that there is irreversible information loss when compressing the data. Lossless compression is less important for data such as images and videos, but it is critical for label data such as numerical labels, binary masks, and segmentation data.\n",
        "\n",
        "\n",
        "**Memory -** Different compression techniques have substantially different memory footprints. For instance, png vs jpeg compression may result in a 10X difference in the size of a Hub dataset. \n",
        "\n",
        "\n",
        "**Runtime -** The primary variables affecting download and upload speeds for generating usable data are the network speed and available compute power for processing the data . In most cases, the network speed is the limiting factor. Therefore, the highest end-to-end throughput for non-local applications is achieved by maximizing compression and utilizing compute power to decompress/convert the data to formats that are consumed by deep learning models (i.e. arrays). \n",
        "\n",
        "\n",
        "**Upload Considerations -** When applicable, the highest uploads speeds can be achieved when the  `sample_compression` input matches the compression of the source data, such as:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkJKv00UFexo"
      },
      "source": [
        "# sample_compression is \"jpg\" and appended image is \"jpeg\"\n",
        "ds.create_tensor('images_jpg', htype = 'image', sample_compression = 'jpg')\n",
        "ds.images_jpg.append(hub.read('./animals/dogs/image_3.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LMsd3K9GJJ9"
      },
      "source": [
        "In this case, the input data is a `.jpg`, and the hub `sample_compression` is `jpg`. \n",
        "\n",
        "However, a mismatch between compression of the source data and sample_compression in Hub results in significantly slower upload speeds, because Hub must decompress the source data and recompress it using the specified `sample_compression` before saving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5MaC1lBwa3a"
      },
      "source": [
        "# sample_compression is \"jpg\" and appended image is \"jpeg\"\n",
        "ds.create_tensor('images_png', htype = 'image', sample_compression = 'png')\n",
        "ds.images_png.append(hub.read('./animals/dogs/image_3.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXGCWxu7wjHX"
      },
      "source": [
        "**NOTE:** Due to the computational costs associated with decompressing and recompressing data, it is important that you consider the runtime implications of uploading source data that is compressed differently than the specified sample_compression. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGo-E8Z8Ho6F"
      },
      "source": [
        "# **Step 4**: _Accessing Data_\n",
        "_Accessing and loading Hub Datasets._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8Mye_Z5Htut"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DI_D7flHvEN"
      },
      "source": [
        "Hub Datasets can be loaded and created in a variety of storage locations with minimal configuration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9dl3mfENulO"
      },
      "source": [
        "import hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sltdan65HmRN"
      },
      "source": [
        "# Local Filepath\n",
        "ds = hub.load('./animals_hub') # Dataset created in Step 2 in this Colab Notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41FBvx25NWMN"
      },
      "source": [
        "# S3\n",
        "# ds = hub.load('s3://my_dataset_bucket', creds={...})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuacdMOgNNmT"
      },
      "source": [
        "# Public Dataset hosted by Activeloop\n",
        "## Activeloop Storage - See Step 6\n",
        "ds = hub.load('hub://activeloop/k49-train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocs18sNqNQfG"
      },
      "source": [
        "# Dataset in another workspace on Activeloop Platform\n",
        "# ds = hub.load('hub://workspace_name/dataset_name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD60qFaAH2qg"
      },
      "source": [
        "**Note:** Since `ds = hub.dataset(path)` can be used to both create and load datasets, you may accidentally create a new dataset if there is a typo in the path you provided while intending to load a dataset. If that occurs, simply use `ds.delete()` to remove the unintended dataset permanently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kb9q_ZqIARN"
      },
      "source": [
        "## Referencing Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq5WSI5LIClV"
      },
      "source": [
        "Hub allows you to reference specific tensors using keys or via the `.` notation outlined below. \n",
        "\n",
        "\n",
        "**Note:** data is still not loaded by these commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr_ZEtBnN1Wp"
      },
      "source": [
        "ds = hub.dataset('hub://activeloop/k49-train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24trRqlLH0Tl"
      },
      "source": [
        "### NO HIERARCHY ###\n",
        "ds.images # is equivalent to\n",
        "ds['images']\n",
        "\n",
        "ds.labels # is equivalent to\n",
        "ds['labels']\n",
        "\n",
        "### WITH HIERARCHY ###\n",
        "# ds.localization.boxes # is equivalent to\n",
        "# ds['localization/boxes']\n",
        "\n",
        "# ds.localization.labels # is equivalent to\n",
        "# ds['localization/labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjmnRLWHINXG"
      },
      "source": [
        "## Accessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js3jsmBHIPqu"
      },
      "source": [
        "Data within the tensors is loaded and accessed using the `.numpy()` command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QUWjQNGILWQ"
      },
      "source": [
        "# Indexing\n",
        "img = ds.images[0].numpy()              # Fetch the 1st image and return a NumPy array\n",
        "label = ds.labels[0].numpy(aslist=True) # Fetch the 1st label and store it as a \n",
        "                                        # as a list\n",
        "                                    \n",
        "# frame = ds.videos[0][4].numpy()   # Fetch the 5th frame in the 1st video \n",
        "                                    # and return a NumPy array\n",
        "                              \n",
        "text_labels = ds.labels[0].data()['text'] # Fetch the first labels and return them as text\n",
        "\n",
        "# Slicing\n",
        "imgs = ds.images[0:100].numpy() # Fetch 100 images and return a NumPy array\n",
        "                                # The method above produces an exception if \n",
        "                                # the images are not all the same size\n",
        "\n",
        "labels = ds.labels[0:100].numpy(aslist=True) # Fetch 100 labels and store \n",
        "                                             # them as a list of NumPy arrays"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DykgrsBEIfk1"
      },
      "source": [
        "**Note:** The `.numpy()` method will produce an exception if all samples in the requested tensor do not have a uniform shape. If that's the case, running `.numpy(aslist=True)` solves the problem by returning a list of NumPy arrays, where the indices of the list correspond to different samples. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 5**: *Visualizing Datasets*"
      ],
      "metadata": {
        "id": "K385Fpvmqc0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of Hub's core features is to enable users to visualize and interpret large amounts of data. Let's load the COCO dataset, which is one of the most popular datasets in computer vision."
      ],
      "metadata": {
        "id": "uSIK-TCAqqQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hub\n",
        "\n",
        "ds = hub.load('hub://activeloop/coco-train')"
      ],
      "metadata": {
        "id": "_YRBC6ehqpgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tensor layout for this dataset can be inspected using:"
      ],
      "metadata": {
        "id": "o5TW4f4Zqzlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds.summary()"
      ],
      "metadata": {
        "id": "YU10NNvNqz54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset can be [visualized in Platform](https://app.activeloop.ai/activeloop/coco-train), or using an iframe in a jupyter notebook:"
      ],
      "metadata": {
        "id": "StDTRjIJq3qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds.visualize()"
      ],
      "metadata": {
        "id": "7G3X22Tdq5tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Visualizing datasets in [Activeloop Platform](https://app.activeloop.ai/) will unlock more features and faster performance compared to visualization in Jupyter notebooks."
      ],
      "metadata": {
        "id": "L713rdfJtVKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualizing your own datasets"
      ],
      "metadata": {
        "id": "ul8Q0CK6rH50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any hub dataset can be visualized using the methods above as long as it follows the conventions necessary for the visualization engine to interpret and parse the data. These conventions [are explained here](https://docs.activeloop.ai/dataset-visualization)."
      ],
      "metadata": {
        "id": "DQzmJazJrOaF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQipSo2OF_lB"
      },
      "source": [
        "# **Step 6**: _Using Activeloop Storage_\n",
        "\n",
        "_Storing and loading datasets from Activeloop Platform Storage._"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register"
      ],
      "metadata": {
        "id": "2TJfXx2pgG7P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA39G647GHX4"
      },
      "source": [
        "You can store your Hub Datasets with Activeloop by first creating an account in [Activeloop Platform](https://app.activeloop.ai/) or in the CLI using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCDC-5dmGFdJ"
      },
      "source": [
        "!activeloop register"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login"
      ],
      "metadata": {
        "id": "o-nf5Sb-gMED"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1iZpxtOGJ0N"
      },
      "source": [
        "In order for the Python API to authenticate with the Activeloop Platform, you should log in from the CLI using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0OUCCMGGLv0"
      },
      "source": [
        "!activeloop login  # prompts for inputting username and password will follow ...\n",
        "\n",
        "# Alternatively, you can directly input your username and password in the same line:\n",
        "# !activeloop login -u my_username -p my_password\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvBxhaAYGNOi"
      },
      "source": [
        "You can then access or create Hub Datasets by passing the Activeloop Platform path to `hub.dataset()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeL0a2zwGXeU"
      },
      "source": [
        "import hub\n",
        "\n",
        "# platform_path = 'hub://workspace_name/dataset_name'\n",
        "#                 'hub://jane_smith/my_awesome_dataset'\n",
        "               \n",
        "ds = hub.dataset(platform_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huQQ1M8kGcyL"
      },
      "source": [
        "**Note**: When you create an account in Activeloop Platform, a default workspace is created that has the same name as your username. You are also able to create other workspaces that represent organizations, teams, or other collections of multiple users. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUdVLQUGGnsA"
      },
      "source": [
        "Public datasets such as `hub://activeloop/mnist-train` can be accessed without logging in."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokens"
      ],
      "metadata": {
        "id": "vgfj-ldqgZa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have an Activeloop account, you can create tokens in [Activeloop Platform](https://app.activeloop.ai/) (Organization Details -> API Tokens) and pass them to python commands that require authentication using:"
      ],
      "metadata": {
        "id": "HhguQ8IxgeBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ds = hub.load(platform_path, token = 'xyz')"
      ],
      "metadata": {
        "id": "sxETFtMlgw0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVma__gxGq97"
      },
      "source": [
        "# **Step 7**: _Connecting Hub Datasets to ML Frameworks_\n",
        "\n",
        "_Connecting Hub Datasets to machine learning frameworks such as PyTorch and TensorFlow._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r-AkeJMGwxB"
      },
      "source": [
        "Hub Datasets can be connected to popular ML frameworks such as PyTorch and TensorFlow using minimal boilerplate code. Our methods enable you to train models while streaming data from the cloud without bottlenecking the training process!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnr9ItdkGzDk"
      },
      "source": [
        "## PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two syntaxes that can be used to train models in Pytorch using Hub datasets:\n",
        "\n",
        "\n",
        "1.   **Hub Data Loaders** are highly-optimized and unlock the fastest streaming and shuffling using hub's internal shuffling method. However, they do not support custom sampling or fully-random shuffling that is possible using PyTorch datasets + data loaders.\n",
        "2.   **Pytorch Datasets + Data Loaders** enable all the customizability supported by PyTorch. However, they have highly sub-optimal streaming using Hub datasets and may result in 5X+ slower performance compared to using Hub data loaders."
      ],
      "metadata": {
        "id": "_IEBtwtU5wdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using Hub Data Loaders**\n",
        "\n",
        "**Best option for fast streaming!**\n"
      ],
      "metadata": {
        "id": "lNPlnk7V58qE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKkrCv2NG1GG"
      },
      "source": [
        "The fastest streaming of data to GPUs using PyTorch is achieved using Hub's built-in PyTorch dataloader `ds.pytorch()`. If your model training is highly sensitive to the randomization of the input data, please pre-shuffle the data, or explore our writeup on [shuffling in ds.pytorch()](https://docs.activeloop.ai/how-hub-works/shuffling-in-ds.pytorch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP3C2uoAGnNK"
      },
      "source": [
        "import hub\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "ds = hub.dataset('hub://activeloop/cifar100-train') # Hub Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3J24ptPAyTw"
      },
      "source": [
        "The transform parameter in `ds.pytorch()` is a dictionary where the `key` is the tensor name and the `value` is the transformation function that should be applied to that tensor. If a specific tensor's data does not need to be returned, it should be omitted from the keys. If a tensor's data does not need to be modified during preprocessing, the transformation function is set as `None`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transform syntax #1 - For independent transforms per tensor"
      ],
      "metadata": {
        "id": "p63ojuuE6jK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transform parameter in `ds.pytorch()` is a dictionary where the key is the tensor name and the value is the transformation function for that tensor. If a tensor's data does not need to be returned, the tensor should be omitted from the keys. If a tensor's data does not need to be modified during preprocessing, the transformation function for the tensor is set as `None`."
      ],
      "metadata": {
        "id": "TITOfIgV6xTE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvqbqsCnA4P3"
      },
      "source": [
        "tform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Must convert to PIL image for subsequent operations to run\n",
        "    transforms.RandomRotation(20), # Image augmentation\n",
        "    transforms.ToTensor(), # Must convert to pytorch tensor for subsequent operations to run\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "#PyTorch Dataloader\n",
        "dataloader= ds.pytorch(batch_size = 16, num_workers = 2, \n",
        "    transform = {'images': tform, 'labels': None}, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transform syntax #2 - For complex or dependent transforms per tensor"
      ],
      "metadata": {
        "id": "29RXFgme6-Gz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform are sometimes more complex where the same transform might need to be applied to all tensors, or tensors need to be combined in a transform. In this case, you can use the syntax below to perform the exact same transform as above:"
      ],
      "metadata": {
        "id": "oXrPn-cH7Bhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(sample_in):\n",
        "    return {'images': tform(sample_in['images']), 'labels': sample_in['labels']}\n",
        "\n",
        "#PyTorch Dataloader\n",
        "dataloader= ds.pytorch(batch_size = 16, num_workers = 2, \n",
        "    transform = transform, shuffle = True)"
      ],
      "metadata": {
        "id": "E9gw49wS69yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Some datasets such as imagenet contain both grayscale and color images, which can cause errors when the transformed images are passed to the model. To convert only the grayscale images to color format, you can add this Torchvision transform to your pipeline:"
      ],
      "metadata": {
        "id": "-EQ2LUPydfPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms.Lambda(lambda x: x.repeat(int(3/x.shape[0]), 1, 1))"
      ],
      "metadata": {
        "id": "3Wi0K1aVdjMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using PyTorch Datasets + Data Loaders**\n",
        "\n",
        "**Best option for full customizability.**"
      ],
      "metadata": {
        "id": "zTsEinAP7HVg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX_MIn_rA70v"
      },
      "source": [
        "Hub datasets can be integrated in the PyTorch Dataset class by passing the `ds` object to the PyTorch Dataset's constructor and pulling data in the `__getitem__` method using `self.ds.image[ids].numpy()`:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, ds, transform = None):\n",
        "        self.ds = ds\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = self.ds.images[idx].numpy()\n",
        "        label = self.ds.labels[idx].numpy(fetch_chunks = True).astype(np.int32)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        sample = {\"images\": image, \"labels\": label}\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "eJJ5nXL98XoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** When loading data sequentially, or when randomly loading samples from a tensor that fits into the cache (such as `class_labels`) it is recommended to set `fetch_chunks = True`. This increases the data loading speed by avoiding separate requests for each individual sample. This is not recommended when randomly loading large tensors, because the data is deleted from the cache before adjacent samples from a chunk are used."
      ],
      "metadata": {
        "id": "ywh5-pRrHcQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PyTorch dataset + data loader is instantiated using the built-in PyTorch functions:"
      ],
      "metadata": {
        "id": "O6o8zPw68iEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_pytorch = ClassificationDataset(ds, transform = tform)\n",
        "\n",
        "dataloader_pytorch = DataLoader(dataset_pytorch, batch_size = 16, num_workers = 2, shuffle = True)"
      ],
      "metadata": {
        "id": "nu39oRbt8q5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Iteration and Training**\n"
      ],
      "metadata": {
        "id": "vUQR2ToB8yWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can iterate through both data loaders above using the exact same syntax. Loading the first batch of data using the Hub data loader may take up to 30 seconds because the [shuffle buffer](https://docs.activeloop.ai/how-hub-works/shuffling-in-ds.pytorch) is filled before any data is returned."
      ],
      "metadata": {
        "id": "EV1e8vrq88UP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuowobdbA96I"
      },
      "source": [
        "for data in dataloader:\n",
        "    print(data)    \n",
        "    break\n",
        "    \n",
        "    # Training Loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in dataloader_pytorch:\n",
        "    print(data)    \n",
        "    break\n",
        "    \n",
        "    # Training Loop"
      ],
      "metadata": {
        "id": "xI5FWDdZ9HCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information on training, check out the tutorial on [Training and Image Classification Model in PyTorch](https://docs.activeloop.ai/hub-tutorials/training-an-image-classification-model-in-pytorch)"
      ],
      "metadata": {
        "id": "H6cutnUI9Y-X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5bX92ZUG_2F"
      },
      "source": [
        "## TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeRUG-arHP1F"
      },
      "source": [
        "Hub Datasets can be converted to TensorFlow Datasets using `ds.tensorflow()`. Downstream, functions from the `tf.Data` API such as map, shuffle, etc. can be applied to process the data before training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1bma0HSHOAO"
      },
      "source": [
        "ds # Hub Dataset object, to be used for training\n",
        "ds_tf = ds.tensorflow() # A TensorFlow Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "76NBB-5vNJcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guao84xTb4Zg"
      },
      "source": [
        "# **Step 8**: _Parallel Computing_\n",
        "\n",
        "_Running computations and processing data in parallel._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVcZ28epcKRc"
      },
      "source": [
        "Hub enables you to easily run computations in parallel and significantly accelerate your data processing workflows. This example primarily focuses on parallel dataset uploading, and other use cases such as dataset transformations can be found in [this tutorial](https://docs.activeloop.ai/hub-tutorials/data-processing-using-parallel-computing).\n",
        "\n",
        "Parallel compute using Hub has two core elements: #1. defining a function or pipeline that will run in parallel and #2. evaluating it using the appropriate inputs and outputs. Let's start with #1 by defining a function that processes files and appends their data to the labels and images tensors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWNxzF1pcWxn"
      },
      "source": [
        "**Defining the parallel computing function**\n",
        "\n",
        "The first step for running parallel computations is to define a function that will run in parallel by decorating it using `@hub.compute`. In the example below, `file_to_hub` converts data from files into hub format, just like in **Step 2: Creating Hub Datasets Manually**. If you have not completed Step 2, please complete the section that downloads and unzips the *animals* dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMjMF_-LcHtl"
      },
      "source": [
        "import hub\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "@hub.compute\n",
        "def file_to_hub(file_name, sample_out, class_names):\n",
        "    ## First two arguments are always default arguments containing:\n",
        "    #     1st argument is an element of the input iterable (list, dataset, array,...)\n",
        "    #     2nd argument is a dataset sample\n",
        "    # Other arguments are optional\n",
        "    \n",
        "    # Find the label number corresponding to the file\n",
        "    label_text = os.path.basename(os.path.dirname(file_name))\n",
        "    label_num = class_names.index(label_text)\n",
        "    \n",
        "    # Append the label and image to the output sample\n",
        "    sample_out.labels.append(np.uint32(label_num))\n",
        "    sample_out.images.append(hub.read(file_name))\n",
        "    \n",
        "    return sample_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-ZhXH-pcgT8"
      },
      "source": [
        "In all functions decorated using `@hub.compute`, the first argument must be a single element of any input iterable that is being processed in parallel. In this case, that is a filename `file_name`, becuase `file_to_hub` reads image files and populates data in the dataset's tensors. \n",
        "\n",
        "The second argument is a dataset sample `sample_out`, which can be operated on using similar syntax to dataset objects, such as `sample_out.append(...)`, `sample_out.extend(...)`, etc.\n",
        "\n",
        "The function decorated using `@hub.compute` must return `sample_out`, which represents the data that is added or modified by that function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIUiNuQqchnH"
      },
      "source": [
        "**Executing the transform**\n",
        "\n",
        "To execute the transform, you must define the dataset that will be modified by the parallel computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZfEn1g_cno_"
      },
      "source": [
        "ds = hub.empty('./animals_hub_transform') # Creates the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7FIReeLcpka"
      },
      "source": [
        "Next, you define the input iterable that describes the information that will be operated on in parallel. In this case, that is a list of files `files_list` from the animals dataset in Step 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CwypbTxcrx0"
      },
      "source": [
        "# Find the class_names and list of files that need to be uploaded\n",
        "dataset_folder = './animals'\n",
        "\n",
        "class_names = os.listdir(dataset_folder)\n",
        "\n",
        "files_list = []\n",
        "for dirpath, dirnames, filenames in os.walk(dataset_folder):\n",
        "    for filename in filenames:\n",
        "        files_list.append(os.path.join(dirpath, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IC-VRKVcuRI"
      },
      "source": [
        "You can now create the tensors for the dataset and **run the parallel computation** using the `.eval` syntax. Pass the optional input arguments to `file_to_hub`, and we skip the first two default arguments `file_name` and `sample_out`. \n",
        "\n",
        "The input iterable `files_list` and output dataset `ds` is passed to the `.eval` method as the first and second argument respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4H4Fug0cxJG"
      },
      "source": [
        "with ds:\n",
        "    ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
        "    ds.create_tensor('labels', htype = 'class_label', class_names = class_names)\n",
        "    \n",
        "    file_to_hub(class_names=class_names).eval(files_list, ds, num_workers = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfWc3_fkhr0W"
      },
      "source": [
        "Image.fromarray(ds.images[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xTj7kt0jrd3"
      },
      "source": [
        "Congrats! You just created a dataset using parallel computing! ðŸŽˆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXRCphquSFs3"
      },
      "source": [
        "# **Step 9**: _Dataset Version Control_\n",
        "\n",
        "*Managing changes to your datasets using Version Control.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y_V53L8SCuB"
      },
      "source": [
        "Hub dataset version control allows you to manage changes to datasets with commands very similar to Git. It provides critical insights into how your data is evolving, and it works with datasets of any size!\n",
        "\n",
        "Let's check out how dataset version control works in Hub! If you haven't done so already, please download and unzip the *animals* dataset from **Step 2**. \n",
        "\n",
        "First let's create a hub dataset in the `./version_control_hub` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgEWowxySUDL"
      },
      "source": [
        "import hub\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Set overwrite = True for re-runability\n",
        "ds = hub.dataset('./version_control_hub', overwrite = True)\n",
        "\n",
        "# Create a tensor and add an image\n",
        "with ds:\n",
        "    ds.create_tensor('images', htype = 'image', sample_compression = 'jpeg')\n",
        "    ds.images.append(hub.read('./animals/cats/image_1.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNLh-JE5pkS_"
      },
      "source": [
        "The first image in this dataset is a picture of a cat:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4hVjQaVpksW"
      },
      "source": [
        "Image.fromarray(ds.images[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CEF-kjySdLp"
      },
      "source": [
        "##Commit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joKq3VV0SdEW"
      },
      "source": [
        "To commit the data added above, simply run `ds.commit`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj9uTZeSTGwT"
      },
      "source": [
        "first_commit_id = ds.commit('Added image of a cat')\n",
        "\n",
        "print('Dataset in commit {} has {} samples'.format(first_commit_id, len(ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc2-MRmaSc4x"
      },
      "source": [
        "Next, let's add another image and commit the update:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zArtG0phTZRv"
      },
      "source": [
        "with ds:\n",
        "    ds.images.append(hub.read('./animals/dogs/image_3.jpg'))\n",
        "    \n",
        "second_commit_id = ds.commit('Added an image of a dog')\n",
        "\n",
        "print('Dataset in commit {} has {} samples'.format(second_commit_id, len(ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYjnY_1RTcjM"
      },
      "source": [
        "The second image in this dataset is a picture of a dog:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbPu9JoFp0ap"
      },
      "source": [
        "Image.fromarray(ds.images[1].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWvgUH25Tj8V"
      },
      "source": [
        "##Log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiqOb8POTkb4"
      },
      "source": [
        "The commit history starting from the current commit can be show using `ds.log`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQSxvzIcTuU-"
      },
      "source": [
        "log = ds.log()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgefyAuATwi4"
      },
      "source": [
        "This command prints the log to the console and also assigns it to the specified variable log. The author of the commit is the username of the [Activeloop account](https://docs.activeloop.ai/getting-started/using-activeloop-storage) that logged in on the machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JRpqeYqV-oT"
      },
      "source": [
        "##Branch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TWcOT4RV-d4"
      },
      "source": [
        "Branching takes place by running the `ds.checkout` command with the parameter `create = True`. Let's create a new branch `dog_flipped`, flip the second image (dog), and create a new commit on that branch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY-CZmzrXr0X"
      },
      "source": [
        "ds.checkout('dog_flipped', create = True)\n",
        "\n",
        "with ds:\n",
        "    ds.images[1] = np.transpose(ds.images[1], axes=[1,0,2])\n",
        "\n",
        "flipped_commit_id = ds.commit('Flipped the dog image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUUMXFKEXuIq"
      },
      "source": [
        "The dog image is now flipped and the log shows a commit on the `dog_flipped` branch as well as the previous commits on `main`: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIP6V3VFqPKS"
      },
      "source": [
        "Image.fromarray(ds.images[1].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3-UgHZPX_0u"
      },
      "source": [
        "ds.log()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrKgp6FYDG9"
      },
      "source": [
        "##Checkout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07nHcIIiYFtW"
      },
      "source": [
        "A previous commit of branch can be checked out using `ds.checkout`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZe8iXjlYEdf"
      },
      "source": [
        "ds.checkout('main')\n",
        "\n",
        "Image.fromarray(ds.images[1].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AZXuEVYYVHm"
      },
      "source": [
        "As expected, the dog image on `main` is not flipped."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diff"
      ],
      "metadata": {
        "id": "gmydIxas3XsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding changes between commits is critical for managing the evolution of datasets. Hub's `ds.diff` function enables users to determine the number of samples that were added, removed, or updated for each tensor. The function can be used in 3 ways:"
      ],
      "metadata": {
        "id": "dTEuB-4C3a-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds.diff() # Diff between the current state and the last commit"
      ],
      "metadata": {
        "id": "XhlPmK9E37Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.diff(first_commit_id) # Diff between the current state and a specific commit"
      ],
      "metadata": {
        "id": "tCa8-nlJ4Dxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.diff(second_commit_id, first_commit_id) # Diff between two specific commits"
      ],
      "metadata": {
        "id": "Bj2Yez624Ecb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1GqH1JvYkNP"
      },
      "source": [
        "##HEAD Commit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbiRZ0eGiBrz"
      },
      "source": [
        "Unlike Git, Hub's version control does not have a staging area because changes to datasets are not stored locally before they are committed. All changes are automatically reflected in the dataset's permanent storage (local or cloud). **Therefore, any changes to a dataset are automatically stored in a HEAD commit on the current branch**. This means that the uncommitted changes do not appear on other branches. Let's see how this works:\n",
        "\n",
        "You should currently be on the `main` branch, which has 2 samples. Let's adds another image:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwuzyJUViZC6"
      },
      "source": [
        "print('Dataset on {} branch has {} samples'.format('main', len(ds)))\n",
        "\n",
        "with ds:\n",
        "    ds.images.append(hub.read('./animals/dogs/image_4.jpg'))\n",
        "    \n",
        "print('After updating, the HEAD commit on {} branch has {} samples'.format('main', len(ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3qePpVFqkG9"
      },
      "source": [
        "The 3rd sample is also an image of a dog:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDfKKuhLqlMM"
      },
      "source": [
        "Image.fromarray(ds.images[2].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4brOnBdyiq6p"
      },
      "source": [
        "Next, if you checkout `dog_flipped` branch, the dataset contains 2 samples, which is sample count from when that branch was created. Therefore, the additional uncommitted third sample that was added to the `main` branch above is not reflected when other branches or commits are checked out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvG-X9VqipM3"
      },
      "source": [
        "ds.checkout('dog_flipped')\n",
        "\n",
        "print('Dataset in {} branch has {} samples'.format('dog_flipped', len(ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aoAeA7vixsC"
      },
      "source": [
        "Finally, when checking our the `main` branch again, the prior uncommitted changes and visible and they are stored in the `HEAD` commit on `main`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DnXiwTmi6G9"
      },
      "source": [
        "ds.checkout('main')\n",
        "\n",
        "print('Dataset in {} branch has {} samples'.format('main', len(ds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztVUV_BDqyHR"
      },
      "source": [
        "The dataset now contains 3 samples and the uncommitted dog image is visible:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci0IHCP9q0In"
      },
      "source": [
        "Image.fromarray(ds.images[2].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uinXs4r1i7Zz"
      },
      "source": [
        "##Merge - Coming Soon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQOGilvkjG2c"
      },
      "source": [
        "Merging is a critical feature for collaborating on datasets, and Activeloop is currently working on an implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz15ukH5jiIm"
      },
      "source": [
        "Congrats! You just are now an expert in dataset version control!ðŸŽ“"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 10:** *Dataset Filtering*"
      ],
      "metadata": {
        "id": "vBM1DKntaXwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering and querying is an important aspect of data engineering because analyzing and utilizing data in smaller units is much more productive than executing workflows on all data all the time. \n",
        "\n",
        "Queries can be performed in Hub enables with user-defined functions, or they can be executed in [Activeloop Platform](https://app.activeloop.ai/) using our highly-performance SQL-style query language."
      ],
      "metadata": {
        "id": "W8K5WREdaf--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering using our SQL query language\n"
      ],
      "metadata": {
        "id": "rv8lkeCNl_Dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Activeloop Platform](https://app.activeloop.ai/) offers a highly-performant SQL-style query language that is built in C++ and is optimized for Hub datasets. Queries and their results are executed and saved in the UI, and they can be accessed in Hub using using the the Dataset Views API described below.\n",
        "Full details about the query language are described in a [standalone tutorial](https://docs.activeloop.ai/hub-tutorials/running-queries)."
      ],
      "metadata": {
        "id": "nk4FHtc2mBlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering with user-defined-functions"
      ],
      "metadata": {
        "id": "Rp9i4Flsai0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step for querying using UDFs is to define a function that returns a boolean depending on whether an input sample in a dataset meets the user-defined condition. In this example, we define a function that returns `True` if the labels for a tensor are in the desired `labels_list`. If there are inputs to the filtering function other than `sample_in`, it must be decorated with `@hub.compute`."
      ],
      "metadata": {
        "id": "ZtwDPVwmapjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hub\n",
        "from PIL import Image\n",
        "\n",
        "# Let's create a local copy of the dataset (Explanation is in the next section)\n",
        "ds = hub.deepcopy('hub://activeloop/mnist-train', './mnist-train-local') "
      ],
      "metadata": {
        "id": "jlIFstX5mrPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_list = ['0', '8'] # Desired labels for filtering\n",
        "\n",
        "@hub.compute\n",
        "def filter_labels(sample_in, labels_list):\n",
        "    \n",
        "    return sample_in.labels.data()['text'][0] in labels_list"
      ],
      "metadata": {
        "id": "2lWTjNbUamyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The filtering function is executed using the `ds.filter()` command below, and it returns a `Dataset View` that only contains the indices that met the filtering condition (more details below). Just like in the Parallel Computing API, the sample_in parameter does not need to be passed into the filter function when evaluating it, and multi-processing can be specified using the `scheduler` and `num_workers` parameters."
      ],
      "metadata": {
        "id": "ccAa1QuZazC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_view = ds.filter(filter_labels(labels_list), scheduler = 'threaded', num_workers = 0)"
      ],
      "metadata": {
        "id": "mq4e5gRIbZ74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ds_view))"
      ],
      "metadata": {
        "id": "ZyJn8DZ3Z0wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** in most cases, multi-processing is not necessary for queries that involve simple data such as labels or bounding boxes. However, multi-processing significantly accelerates queries that must load rich data types such as images and videos."
      ],
      "metadata": {
        "id": "5ibjQKAEbs9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Views"
      ],
      "metadata": {
        "id": "L4_yT5ZunQcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Dataset View is any subset of a Hub dataset that does not contains all of the samples. It can be an output of a query, filtering function, or regular indexing like `ds[0:2:100]`."
      ],
      "metadata": {
        "id": "cqArYgDZnWcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** In the filtering example above, we copied mnist-train locally in order to gain write access to the dataset, which affects the saving of `Dataset Views`. With write access, the views are saved as part of the dataset. Without write access, views are stored elsewhere or in custom paths, and full details are available here. Users have write access to their own datasets, regardless of whether the datasets are local or in the cloud."
      ],
      "metadata": {
        "id": "Tc7Y_28lnWWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data in the returned `ds_view` can be accessed just like a regular dataset."
      ],
      "metadata": {
        "id": "G2DowEHfbb2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image.fromarray(ds_view.images[0].numpy())"
      ],
      "metadata": {
        "id": "4FCxLH0Nbec8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `Dataset View` can be saved permanently using the method below, which stores its indices without copying the data:"
      ],
      "metadata": {
        "id": "p8NxyIw2nnWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_view.save_view(message = 'Samples with 0 and 8')"
      ],
      "metadata": {
        "id": "uzmlwsJLnmfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** In order to maintain data lineage, Dataset Views are immutable and are connected to specific commits. Therefore, views can only be saved if the dataset has a commit and there are no uncommitted changes in the HEAD"
      ],
      "metadata": {
        "id": "KyaFzRE6n50w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each `Dataset View` has a unique `id`, and views can be examined or loaded using:"
      ],
      "metadata": {
        "id": "aLB1sM50Y4ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "views = ds.get_views()\n",
        "\n",
        "print(views)"
      ],
      "metadata": {
        "id": "nzQ1foyWY3yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_view = views[-1].load()\n",
        "\n",
        "# OR\n",
        "\n",
        "# ds_view = ds.load_view(id)"
      ],
      "metadata": {
        "id": "0PKOwe5qY8zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ds_view))"
      ],
      "metadata": {
        "id": "tVuY_x35anoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats! You just learned to filter adn query data with Hub! ðŸŽˆ"
      ],
      "metadata": {
        "id": "KEi-DxIOcpHp"
      }
    }
  ]
}